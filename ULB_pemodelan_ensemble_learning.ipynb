{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ULB_pemodelan_ensemble_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azizeko29/new_deteksi_fraud/blob/master/ULB_pemodelan_ensemble_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "qhorwCS90pU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load in \n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "# import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "import math\n",
        "import collections\n",
        "import time\n",
        "from scipy import stats"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "JVakcQRs0pVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %matplotlib inline\n",
        "# from IPython.display import display\n",
        "# from sklearn import metrics\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from statistics import stdev\n",
        "\n",
        "from sklearn import tree\n",
        "\n",
        "from IPython.display import Image  \n",
        "from sklearn.externals.six import StringIO  \n",
        "import pydotplus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2iQeEbs0pVP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# np.random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MfeBiCN0pVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def confussion_matrik(actual,predict):\n",
        "    TP,FP,FN,TN = 0,0,0,0\n",
        "    for i,val in enumerate(actual):\n",
        "        if val == 0:\n",
        "            if val == predict[i]:\n",
        "                TN += 1\n",
        "            else:\n",
        "                FP += 1\n",
        "        if val == 1:\n",
        "            if val == predict[i]:\n",
        "                TP += 1\n",
        "            else:\n",
        "                FN += 1\n",
        "    return TP,FP,FN,TN\n",
        " \n",
        "def acc_sens_spec(actual,predict):\n",
        "    TP,FP,FN,TN = confussion_matrik(actual,predict)\n",
        "# akurasi\n",
        "    if (TP+FP+FN+TN) == 0 :\n",
        "        accuracy = 0 \n",
        "    else :\n",
        "        accuracy = (TP+TN)/(TP+FP+FN+TN)\n",
        "        \n",
        "# sensitivity\n",
        "    if (TP+FN) == 0 :\n",
        "        sensitivity = 0\n",
        "    else :\n",
        "        sensitivity = TP/(TP+FN)\n",
        "        \n",
        "# specifity    \n",
        "    if (TN +FP) == 0 :\n",
        "        specifity = 0\n",
        "    else :\n",
        "        specifity = TN/(TN +FP)\n",
        "        \n",
        "# precision\n",
        "    if (TP+FP) == 0 :\n",
        "        precision = 0\n",
        "    else :\n",
        "        precision = TP/(TP+FP)\n",
        "\n",
        "# recall\n",
        "    recall = sensitivity\n",
        "\n",
        "# f1_score\n",
        "    if (precision+recall) == 0 :\n",
        "        f1_score = 0\n",
        "    else :\n",
        "        f1_score = 2*((precision*recall)/(precision+recall))  \n",
        "    \n",
        "    return accuracy,sensitivity,specifity,precision,recall,f1_score\n",
        "\n",
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "    #how many correct predictions?\n",
        "    correct = 0\n",
        "    #for each actual label\n",
        "    for i in range(len(actual)):\n",
        "        #if actual matches predicted label\n",
        "        if actual[i] == predicted[i]:\n",
        "            #add 1 to the correct iterator\n",
        "            correct += 1\n",
        "    #return percentage of predictions that were correct\n",
        "    return correct / float(len(actual)) * 100.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx6bm6hU0pVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# confussion_matrik(y,pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY_TWVIp0pVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy,sensitivity,specifity = acc_sens_spec(y,pred)\n",
        "# print('acc',accuracy)\n",
        "# print('sens',sensitivity)\n",
        "# print('spec',specifity)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "F0-pV4LQ0pVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# accuracy_metric(y,RF.predict(X))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGWI9BJv0pVh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cross_val_split(X,fold=2,seed=0):\n",
        "    np.random.seed(seed)\n",
        "    n_folds= fold\n",
        "    size = X.shape[0]/n_folds\n",
        "    X_idx = list(range(X.shape[0]))\n",
        "    folds_data= []\n",
        "    for i in range(n_folds):\n",
        "#         print(X_idx)\n",
        "        random_idx = list(np.random.choice(X_idx,int(size),replace=False))\n",
        "#         print(random_idx)\n",
        "        X_idx = [idx for idx in X_idx if idx not in random_idx]\n",
        "#         print(X_idx)\n",
        "\n",
        "        folds_data.append(random_idx)\n",
        "#         print(\"--\")\n",
        "    return folds_data\n",
        "\n",
        "def kfold_cross_validation(model,X,y, n_fold=2, n_seed=0):\n",
        "    folds = cross_val_split(X,fold=n_fold,seed=n_seed)\n",
        "    fold_result =[]\n",
        "    for i in range(len(folds)):\n",
        "    #     print(i)\n",
        "        train = []\n",
        "        for j in range(len(folds)):\n",
        "            if j != i:\n",
        "                train = train + folds[j]\n",
        "        test = folds[i]\n",
        "\n",
        "        X_train = X.iloc[train,:].reset_index(drop=True)\n",
        "        y_train = y[train].reset_index(drop=True)\n",
        "\n",
        "        X_test = X.iloc[test,:].reset_index(drop=True)\n",
        "        y_test = y[test].reset_index(drop=True)\n",
        "\n",
        "\n",
        "        t0 = time.time()\n",
        "        model.fit(X_train, y_train)\n",
        "        t1 = time.time()\n",
        "        waktu = t1 - t0\n",
        "\n",
        "        predict = model.predict(X_test)\n",
        "        accuracy,sensitivity,specifity,precision,recall,f1_score = acc_sens_spec(y_test,predict)\n",
        "\n",
        "        result = [accuracy,sensitivity,specifity,precision,recall,f1_score,waktu]\n",
        "        fold_result.append(result)\n",
        "        \n",
        "    return fold_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUxofECh0pVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import pylab as pl\n",
        "def plot_this(X_rs,y_rs,method):\n",
        "  # Use principal component to condense the 10 features to 2 features\n",
        "  pca = PCA(n_components=2).fit(X_rs)\n",
        "  pca_2d = pca.transform(X_rs)\n",
        "  # Assign colors\n",
        "  for i in range(0, pca_2d.shape[0]):\n",
        "    if y_rs[i] == 0:\n",
        "      c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r', marker='o')\n",
        "    elif y_rs[i] == 1:\n",
        "      c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g', marker='*')  \n",
        "  pl.legend([c1, c2], ['Class 1', 'Class 2'])\n",
        "  pl.title(method)\n",
        "  pl.axis([-4, 5, -4, 4])  # x axis (-4,5), y axis (-4,4)\n",
        "  pl.show()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_KBJ2Rm0pVq",
        "colab_type": "text"
      },
      "source": [
        "## ---------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNk8CXpR0pVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ##read data\n",
        "\n",
        "# df = pd.read_csv('data/creditcard_ulb.csv',sep=\",\")\n",
        "# # df = pd.read_csv('data/sample_data.csv',sep=\",\")\n",
        "\n",
        "# df0 = df.copy()\n",
        "# df.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2n8i4xh1jqQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "805d0a0a-4ef7-40f5-c798-7fa7028b21d5"
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "link = 'https://drive.google.com/open?id=1PqCaaZwbyxrFdhLLPFB0TM9Z4jcP7LPo' # The shareable link\n",
        "\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('creditcard_ulb.csv')  \n",
        "read_df = pd.read_csv('creditcard_ulb.csv')\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "# df.info()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1PqCaaZwbyxrFdhLLPFB0TM9Z4jcP7LPo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asAMzLKA1wby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = read_df.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3M3tn8i0pVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "e6b13b31-9a96-4c63-ed67-45219ecad157"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>0.090794</td>\n",
              "      <td>-0.551600</td>\n",
              "      <td>-0.617801</td>\n",
              "      <td>-0.991390</td>\n",
              "      <td>-0.311169</td>\n",
              "      <td>1.468177</td>\n",
              "      <td>-0.470401</td>\n",
              "      <td>0.207971</td>\n",
              "      <td>0.025791</td>\n",
              "      <td>0.403993</td>\n",
              "      <td>0.251412</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>-0.166974</td>\n",
              "      <td>1.612727</td>\n",
              "      <td>1.065235</td>\n",
              "      <td>0.489095</td>\n",
              "      <td>-0.143772</td>\n",
              "      <td>0.635558</td>\n",
              "      <td>0.463917</td>\n",
              "      <td>-0.114805</td>\n",
              "      <td>-0.183361</td>\n",
              "      <td>-0.145783</td>\n",
              "      <td>-0.069083</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>0.207643</td>\n",
              "      <td>0.624501</td>\n",
              "      <td>0.066084</td>\n",
              "      <td>0.717293</td>\n",
              "      <td>-0.165946</td>\n",
              "      <td>2.345865</td>\n",
              "      <td>-2.890083</td>\n",
              "      <td>1.109969</td>\n",
              "      <td>-0.121359</td>\n",
              "      <td>-2.261857</td>\n",
              "      <td>0.524980</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>-0.054952</td>\n",
              "      <td>-0.226487</td>\n",
              "      <td>0.178228</td>\n",
              "      <td>0.507757</td>\n",
              "      <td>-0.287924</td>\n",
              "      <td>-0.631418</td>\n",
              "      <td>-1.059647</td>\n",
              "      <td>-0.684093</td>\n",
              "      <td>1.965775</td>\n",
              "      <td>-1.232622</td>\n",
              "      <td>-0.208038</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>0.753074</td>\n",
              "      <td>-0.822843</td>\n",
              "      <td>0.538196</td>\n",
              "      <td>1.345852</td>\n",
              "      <td>-1.119670</td>\n",
              "      <td>0.175121</td>\n",
              "      <td>-0.451449</td>\n",
              "      <td>-0.237033</td>\n",
              "      <td>-0.038195</td>\n",
              "      <td>0.803487</td>\n",
              "      <td>0.408542</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0   0.0 -1.359807 -0.072781  2.536347  ...  0.133558 -0.021053  149.62      0\n",
              "1   0.0  1.191857  0.266151  0.166480  ... -0.008983  0.014724    2.69      0\n",
              "2   1.0 -1.358354 -1.340163  1.773209  ... -0.055353 -0.059752  378.66      0\n",
              "3   1.0 -0.966272 -0.185226  1.792993  ...  0.062723  0.061458  123.50      0\n",
              "4   2.0 -1.158233  0.877737  1.548718  ...  0.219422  0.215153   69.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWjnAx8s0pV4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "327262fa-95a8-4913-ec23-9ca0552b3c77"
      },
      "source": [
        "df.tail()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>284802</th>\n",
              "      <td>172786.0</td>\n",
              "      <td>-11.881118</td>\n",
              "      <td>10.071785</td>\n",
              "      <td>-9.834783</td>\n",
              "      <td>-2.066656</td>\n",
              "      <td>-5.364473</td>\n",
              "      <td>-2.606837</td>\n",
              "      <td>-4.918215</td>\n",
              "      <td>7.305334</td>\n",
              "      <td>1.914428</td>\n",
              "      <td>4.356170</td>\n",
              "      <td>-1.593105</td>\n",
              "      <td>2.711941</td>\n",
              "      <td>-0.689256</td>\n",
              "      <td>4.626942</td>\n",
              "      <td>-0.924459</td>\n",
              "      <td>1.107641</td>\n",
              "      <td>1.991691</td>\n",
              "      <td>0.510632</td>\n",
              "      <td>-0.682920</td>\n",
              "      <td>1.475829</td>\n",
              "      <td>0.213454</td>\n",
              "      <td>0.111864</td>\n",
              "      <td>1.014480</td>\n",
              "      <td>-0.509348</td>\n",
              "      <td>1.436807</td>\n",
              "      <td>0.250034</td>\n",
              "      <td>0.943651</td>\n",
              "      <td>0.823731</td>\n",
              "      <td>0.77</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284803</th>\n",
              "      <td>172787.0</td>\n",
              "      <td>-0.732789</td>\n",
              "      <td>-0.055080</td>\n",
              "      <td>2.035030</td>\n",
              "      <td>-0.738589</td>\n",
              "      <td>0.868229</td>\n",
              "      <td>1.058415</td>\n",
              "      <td>0.024330</td>\n",
              "      <td>0.294869</td>\n",
              "      <td>0.584800</td>\n",
              "      <td>-0.975926</td>\n",
              "      <td>-0.150189</td>\n",
              "      <td>0.915802</td>\n",
              "      <td>1.214756</td>\n",
              "      <td>-0.675143</td>\n",
              "      <td>1.164931</td>\n",
              "      <td>-0.711757</td>\n",
              "      <td>-0.025693</td>\n",
              "      <td>-1.221179</td>\n",
              "      <td>-1.545556</td>\n",
              "      <td>0.059616</td>\n",
              "      <td>0.214205</td>\n",
              "      <td>0.924384</td>\n",
              "      <td>0.012463</td>\n",
              "      <td>-1.016226</td>\n",
              "      <td>-0.606624</td>\n",
              "      <td>-0.395255</td>\n",
              "      <td>0.068472</td>\n",
              "      <td>-0.053527</td>\n",
              "      <td>24.79</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284804</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>1.919565</td>\n",
              "      <td>-0.301254</td>\n",
              "      <td>-3.249640</td>\n",
              "      <td>-0.557828</td>\n",
              "      <td>2.630515</td>\n",
              "      <td>3.031260</td>\n",
              "      <td>-0.296827</td>\n",
              "      <td>0.708417</td>\n",
              "      <td>0.432454</td>\n",
              "      <td>-0.484782</td>\n",
              "      <td>0.411614</td>\n",
              "      <td>0.063119</td>\n",
              "      <td>-0.183699</td>\n",
              "      <td>-0.510602</td>\n",
              "      <td>1.329284</td>\n",
              "      <td>0.140716</td>\n",
              "      <td>0.313502</td>\n",
              "      <td>0.395652</td>\n",
              "      <td>-0.577252</td>\n",
              "      <td>0.001396</td>\n",
              "      <td>0.232045</td>\n",
              "      <td>0.578229</td>\n",
              "      <td>-0.037501</td>\n",
              "      <td>0.640134</td>\n",
              "      <td>0.265745</td>\n",
              "      <td>-0.087371</td>\n",
              "      <td>0.004455</td>\n",
              "      <td>-0.026561</td>\n",
              "      <td>67.88</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284805</th>\n",
              "      <td>172788.0</td>\n",
              "      <td>-0.240440</td>\n",
              "      <td>0.530483</td>\n",
              "      <td>0.702510</td>\n",
              "      <td>0.689799</td>\n",
              "      <td>-0.377961</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>-0.686180</td>\n",
              "      <td>0.679145</td>\n",
              "      <td>0.392087</td>\n",
              "      <td>-0.399126</td>\n",
              "      <td>-1.933849</td>\n",
              "      <td>-0.962886</td>\n",
              "      <td>-1.042082</td>\n",
              "      <td>0.449624</td>\n",
              "      <td>1.962563</td>\n",
              "      <td>-0.608577</td>\n",
              "      <td>0.509928</td>\n",
              "      <td>1.113981</td>\n",
              "      <td>2.897849</td>\n",
              "      <td>0.127434</td>\n",
              "      <td>0.265245</td>\n",
              "      <td>0.800049</td>\n",
              "      <td>-0.163298</td>\n",
              "      <td>0.123205</td>\n",
              "      <td>-0.569159</td>\n",
              "      <td>0.546668</td>\n",
              "      <td>0.108821</td>\n",
              "      <td>0.104533</td>\n",
              "      <td>10.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284806</th>\n",
              "      <td>172792.0</td>\n",
              "      <td>-0.533413</td>\n",
              "      <td>-0.189733</td>\n",
              "      <td>0.703337</td>\n",
              "      <td>-0.506271</td>\n",
              "      <td>-0.012546</td>\n",
              "      <td>-0.649617</td>\n",
              "      <td>1.577006</td>\n",
              "      <td>-0.414650</td>\n",
              "      <td>0.486180</td>\n",
              "      <td>-0.915427</td>\n",
              "      <td>-1.040458</td>\n",
              "      <td>-0.031513</td>\n",
              "      <td>-0.188093</td>\n",
              "      <td>-0.084316</td>\n",
              "      <td>0.041333</td>\n",
              "      <td>-0.302620</td>\n",
              "      <td>-0.660377</td>\n",
              "      <td>0.167430</td>\n",
              "      <td>-0.256117</td>\n",
              "      <td>0.382948</td>\n",
              "      <td>0.261057</td>\n",
              "      <td>0.643078</td>\n",
              "      <td>0.376777</td>\n",
              "      <td>0.008797</td>\n",
              "      <td>-0.473649</td>\n",
              "      <td>-0.818267</td>\n",
              "      <td>-0.002415</td>\n",
              "      <td>0.013649</td>\n",
              "      <td>217.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Time         V1         V2  ...       V28  Amount  Class\n",
              "284802  172786.0 -11.881118  10.071785  ...  0.823731    0.77      0\n",
              "284803  172787.0  -0.732789  -0.055080  ... -0.053527   24.79      0\n",
              "284804  172788.0   1.919565  -0.301254  ... -0.026561   67.88      0\n",
              "284805  172788.0  -0.240440   0.530483  ...  0.104533   10.00      0\n",
              "284806  172792.0  -0.533413  -0.189733  ...  0.013649  217.00      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jN6s3PK0pV-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a15966c8-f984-455b-fa31-a0b673186b41"
      },
      "source": [
        "df['Class'].value_counts()"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    284315\n",
              "1       492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1KQqHE40pWD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "78346a2b-b20c-419e-a558-26746fd7555e"
      },
      "source": [
        "plt.figure(1,figsize=(5, 5), dpi=80)\n",
        "plt.grid(color='b', linestyle='-', linewidth=0.2)\n",
        "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
        "sns.countplot('Class', data=df,palette='Set2')\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe49a3eb080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAGLCAYAAADksLTTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqZR2kcj0pWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a3fea33-e9d0-4854-e4de-8ee3387a05e6"
      },
      "source": [
        "count = 0\n",
        "for a in df.columns[1:]:\n",
        "    if df[a].isnull().sum() > 0 :\n",
        "#         print(a)\n",
        "        count+=1\n",
        "count"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZ0RAj-h0pWK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from imblearn.under_sampling import (RandomUnderSampler, \n",
        "#                                      ClusterCentroids,\n",
        "#                                      TomekLinks,\n",
        "#                                      NeighbourhoodCleaningRule,\n",
        "#                                      NearMiss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkvDxKty0pWO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Since most of our data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n",
        "# from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "\n",
        "# # RobustScaler is less prone to outliers.\n",
        "\n",
        "# std_scaler = StandardScaler()\n",
        "# rob_scaler = RobustScaler()\n",
        "# minmax_scaler = MinMaxScaler()\n",
        "\n",
        "# # df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "# # df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "# df['scaled_amount'] = minmax_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
        "# df['scaled_time'] = minmax_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
        "\n",
        "# df.drop(['Time','Amount'], axis=1, inplace=True)\n",
        "\n",
        "# scaled_amount = df['scaled_amount']\n",
        "# scaled_time = df['scaled_time']\n",
        "\n",
        "# df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
        "# df.insert(0, 'scaled_amount', scaled_amount)\n",
        "# df.insert(1, 'scaled_time', scaled_time)\n",
        "\n",
        "# # Amount and Time are Scaled!\n",
        "\n",
        "# df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUVZsMIB0pWR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "ab1b92fd-f033-4f33-941b-39028df2520e"
      },
      "source": [
        "# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n",
        "\n",
        "# Lets shuffle the data before creating the subsamples\n",
        "df_copy = df.copy()\n",
        "# df_copy = df_copy.sample(frac=1)\n",
        "\n",
        "# amount of fraud classes 492 rows.\n",
        "fraud_df = df_copy.loc[df_copy['Class'] == 1]\n",
        "non_fraud_df = df_copy.loc[df['Class'] == 0][:492]\n",
        "non_fraud_df = df_copy.loc[df_copy['Class'] == 0].sample(n=492,random_state=1)\n",
        "\n",
        "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
        "\n",
        "# Shuffle dataframe rows\n",
        "df_new = normal_distributed_df.sample(frac=1, random_state=1)\n",
        "df_new = df_new.reset_index()\n",
        "df_new = df_new.drop('index', axis=1)\n",
        "df_new.head()"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>V10</th>\n",
              "      <th>V11</th>\n",
              "      <th>V12</th>\n",
              "      <th>V13</th>\n",
              "      <th>V14</th>\n",
              "      <th>V15</th>\n",
              "      <th>V16</th>\n",
              "      <th>V17</th>\n",
              "      <th>V18</th>\n",
              "      <th>V19</th>\n",
              "      <th>V20</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>128627.0</td>\n",
              "      <td>-0.865285</td>\n",
              "      <td>-0.979506</td>\n",
              "      <td>2.587540</td>\n",
              "      <td>-2.781144</td>\n",
              "      <td>-0.887336</td>\n",
              "      <td>-0.579689</td>\n",
              "      <td>-0.976755</td>\n",
              "      <td>0.132058</td>\n",
              "      <td>-1.658263</td>\n",
              "      <td>0.335742</td>\n",
              "      <td>-1.256944</td>\n",
              "      <td>-0.910902</td>\n",
              "      <td>0.767742</td>\n",
              "      <td>-1.122642</td>\n",
              "      <td>-0.540790</td>\n",
              "      <td>0.030130</td>\n",
              "      <td>0.106431</td>\n",
              "      <td>0.327877</td>\n",
              "      <td>-0.874734</td>\n",
              "      <td>-0.211955</td>\n",
              "      <td>-0.106978</td>\n",
              "      <td>-0.010528</td>\n",
              "      <td>-0.211955</td>\n",
              "      <td>0.021026</td>\n",
              "      <td>0.358237</td>\n",
              "      <td>-0.209483</td>\n",
              "      <td>0.062051</td>\n",
              "      <td>0.074730</td>\n",
              "      <td>8.00</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>70536.0</td>\n",
              "      <td>-2.271755</td>\n",
              "      <td>-0.457655</td>\n",
              "      <td>-2.589055</td>\n",
              "      <td>2.230778</td>\n",
              "      <td>-4.278983</td>\n",
              "      <td>0.388610</td>\n",
              "      <td>0.102485</td>\n",
              "      <td>0.813128</td>\n",
              "      <td>-1.092921</td>\n",
              "      <td>-5.032028</td>\n",
              "      <td>3.510348</td>\n",
              "      <td>-2.227398</td>\n",
              "      <td>0.656824</td>\n",
              "      <td>-5.199186</td>\n",
              "      <td>-0.128311</td>\n",
              "      <td>-3.943521</td>\n",
              "      <td>-3.820522</td>\n",
              "      <td>-0.570821</td>\n",
              "      <td>2.783383</td>\n",
              "      <td>2.285758</td>\n",
              "      <td>1.096342</td>\n",
              "      <td>0.658399</td>\n",
              "      <td>1.711676</td>\n",
              "      <td>0.333540</td>\n",
              "      <td>0.538591</td>\n",
              "      <td>-0.193529</td>\n",
              "      <td>0.258194</td>\n",
              "      <td>0.247269</td>\n",
              "      <td>824.83</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>166831.0</td>\n",
              "      <td>-2.027135</td>\n",
              "      <td>-1.131890</td>\n",
              "      <td>-1.135194</td>\n",
              "      <td>1.086963</td>\n",
              "      <td>-0.010547</td>\n",
              "      <td>0.423797</td>\n",
              "      <td>3.790880</td>\n",
              "      <td>-1.155595</td>\n",
              "      <td>-0.063434</td>\n",
              "      <td>1.334414</td>\n",
              "      <td>1.032016</td>\n",
              "      <td>-0.722023</td>\n",
              "      <td>-1.533240</td>\n",
              "      <td>0.334119</td>\n",
              "      <td>0.297479</td>\n",
              "      <td>-0.429392</td>\n",
              "      <td>-0.824644</td>\n",
              "      <td>0.489668</td>\n",
              "      <td>0.873344</td>\n",
              "      <td>0.033804</td>\n",
              "      <td>-0.315105</td>\n",
              "      <td>0.575520</td>\n",
              "      <td>0.490842</td>\n",
              "      <td>0.756502</td>\n",
              "      <td>-0.142685</td>\n",
              "      <td>-0.602777</td>\n",
              "      <td>0.508712</td>\n",
              "      <td>-0.091646</td>\n",
              "      <td>634.30</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>75987.0</td>\n",
              "      <td>0.531678</td>\n",
              "      <td>-1.108844</td>\n",
              "      <td>0.276972</td>\n",
              "      <td>0.386453</td>\n",
              "      <td>-1.038906</td>\n",
              "      <td>-0.810526</td>\n",
              "      <td>0.395582</td>\n",
              "      <td>-0.322635</td>\n",
              "      <td>0.068460</td>\n",
              "      <td>-0.379352</td>\n",
              "      <td>0.027938</td>\n",
              "      <td>0.964578</td>\n",
              "      <td>1.555891</td>\n",
              "      <td>-0.046948</td>\n",
              "      <td>1.142469</td>\n",
              "      <td>0.483455</td>\n",
              "      <td>-0.462960</td>\n",
              "      <td>-0.606466</td>\n",
              "      <td>-0.124976</td>\n",
              "      <td>0.801043</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>-0.824566</td>\n",
              "      <td>-0.174821</td>\n",
              "      <td>0.479535</td>\n",
              "      <td>-0.094335</td>\n",
              "      <td>0.698329</td>\n",
              "      <td>-0.130716</td>\n",
              "      <td>0.083227</td>\n",
              "      <td>386.60</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>136908.0</td>\n",
              "      <td>1.878626</td>\n",
              "      <td>0.162765</td>\n",
              "      <td>-0.167433</td>\n",
              "      <td>3.465196</td>\n",
              "      <td>0.197332</td>\n",
              "      <td>1.157212</td>\n",
              "      <td>-0.676783</td>\n",
              "      <td>0.473890</td>\n",
              "      <td>-0.386278</td>\n",
              "      <td>1.592102</td>\n",
              "      <td>-0.434179</td>\n",
              "      <td>-0.728119</td>\n",
              "      <td>-1.803685</td>\n",
              "      <td>0.416732</td>\n",
              "      <td>-0.806753</td>\n",
              "      <td>1.813132</td>\n",
              "      <td>-1.356800</td>\n",
              "      <td>0.618755</td>\n",
              "      <td>-1.030612</td>\n",
              "      <td>-0.342723</td>\n",
              "      <td>-0.217428</td>\n",
              "      <td>-0.785738</td>\n",
              "      <td>0.406279</td>\n",
              "      <td>-0.056071</td>\n",
              "      <td>-0.560484</td>\n",
              "      <td>-0.388620</td>\n",
              "      <td>-0.012717</td>\n",
              "      <td>-0.038421</td>\n",
              "      <td>5.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time        V1        V2        V3  ...       V27       V28  Amount  Class\n",
              "0  128627.0 -0.865285 -0.979506  2.587540  ...  0.062051  0.074730    8.00      0\n",
              "1   70536.0 -2.271755 -0.457655 -2.589055  ...  0.258194  0.247269  824.83      1\n",
              "2  166831.0 -2.027135 -1.131890 -1.135194  ...  0.508712 -0.091646  634.30      1\n",
              "3   75987.0  0.531678 -1.108844  0.276972  ... -0.130716  0.083227  386.60      0\n",
              "4  136908.0  1.878626  0.162765 -0.167433  ... -0.012717 -0.038421    5.99      0\n",
              "\n",
              "[5 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lt8BRgQR0pWW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpwvc77C0pWa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "outputId": "b83aa953-2235-4fcd-f539-17bedcd9ef1c"
      },
      "source": [
        "plt.figure(1,figsize=(5, 5), dpi=80)\n",
        "plt.grid(color='b', linestyle='-', linewidth=0.2)\n",
        "plt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
        "sns.countplot('Class', data=df_new,palette='Set2')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fe49d4ea9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAGLCAYAAAAvR5JoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJT0QAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJwi_WXQ0pWi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KX3HYPhb0pWm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X = df.drop('Class', axis=1)\n",
        "# y = df['Class'].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caCPe2iv0pWq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_new = df_new.drop('Class', axis=1)\n",
        "y_new = df_new['Class'].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CgFBdJYI0pWy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4587da78-a98a-4167-a243-e6aa3aa8308c"
      },
      "source": [
        "y_new.value_counts()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    492\n",
              "0    492\n",
              "Name: Class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLN-D7aE0pW3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "54f497e5-9675-46ae-9f0f-69891edb25b5"
      },
      "source": [
        "X_new.columns"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
              "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
              "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2w12u1g0pW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "\n",
        "# # RobustScaler is less prone to outliers.\n",
        "\n",
        "# std_scaler = StandardScaler()\n",
        "# rob_scaler = RobustScaler()\n",
        "# minmax_scaler = MinMaxScaler()\n",
        "\n",
        "# X_neww = minmax_scaler.fit_transform(X_new)\n",
        "# X_new = pd.DataFrame(X_neww, columns=X_new.columns)\n",
        "# X_new.tail()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cIQeIF40pXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbJzY3gx0pXH",
        "colab_type": "text"
      },
      "source": [
        "# # SKLearn\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEpIziq60pXI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "247650b3-46de-4546-99c9-5636cef63705"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "n_tree = 10\n",
        "clf = RandomForestClassifier(random_state=1,n_estimators=n_tree, max_features='sqrt')\n",
        "clf.fit(X_new,y_new)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=10,\n",
              "                       n_jobs=None, oob_score=False, random_state=1, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxJsFv4a0pXN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "c07e0cbf-a4c3-4faf-f9b3-af3a882a4c4b"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf_DT = DecisionTreeClassifier(random_state=1)\n",
        "clf_DT.fit(X_new,y_new)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
              "                       max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort=False,\n",
              "                       random_state=1, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTyHeoqt0pXT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_M_uzJJ20pXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_skenarion_1 (clf,X,y,kfolds):\n",
        "    CV_SKLearn = kfold_cross_validation(clf,X,y,n_fold=kfolds,n_seed=1)\n",
        "    df_result = pd.DataFrame(data= CV_SKLearn, columns=['Akurasi','Sensitivity','Specifity','precision','recall','f1_score','Waktu'])\n",
        "\n",
        "    df_result.insert(loc=0, column='No', value=list(range(1,df_result.shape[0]+1)))\n",
        "    df_result = df_result.set_index('No')\n",
        "\n",
        "    del df_result.index.name\n",
        "\n",
        "    df_result = df_result.append(df_result.describe()[1:2])\n",
        "    \n",
        "    return df_result\n",
        "# result_SKLearn = result_SKLearn.append('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrKXkDIe0pXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7a7e9ac9-88a5-475c-a2fb-80ec7521014a"
      },
      "source": [
        "result_SKLearn = run_skenarion_1(clf,X_new,y_new,5)\n",
        "result_SKLearn"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Akurasi</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specifity</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>Waktu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.861702</td>\n",
              "      <td>0.970588</td>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.861702</td>\n",
              "      <td>0.910112</td>\n",
              "      <td>0.026078</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.933673</td>\n",
              "      <td>0.915094</td>\n",
              "      <td>0.955556</td>\n",
              "      <td>0.960396</td>\n",
              "      <td>0.915094</td>\n",
              "      <td>0.937198</td>\n",
              "      <td>0.024908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.964286</td>\n",
              "      <td>0.936842</td>\n",
              "      <td>0.990099</td>\n",
              "      <td>0.988889</td>\n",
              "      <td>0.936842</td>\n",
              "      <td>0.962162</td>\n",
              "      <td>0.025436</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.913265</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.951456</td>\n",
              "      <td>0.941860</td>\n",
              "      <td>0.870968</td>\n",
              "      <td>0.905028</td>\n",
              "      <td>0.034005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.933673</td>\n",
              "      <td>0.891089</td>\n",
              "      <td>0.978947</td>\n",
              "      <td>0.978261</td>\n",
              "      <td>0.891089</td>\n",
              "      <td>0.932642</td>\n",
              "      <td>0.025727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.932653</td>\n",
              "      <td>0.895139</td>\n",
              "      <td>0.969329</td>\n",
              "      <td>0.966738</td>\n",
              "      <td>0.895139</td>\n",
              "      <td>0.929429</td>\n",
              "      <td>0.027231</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Akurasi  Sensitivity  Specifity  precision    recall  f1_score     Waktu\n",
              "1     0.918367     0.861702   0.970588   0.964286  0.861702  0.910112  0.026078\n",
              "2     0.933673     0.915094   0.955556   0.960396  0.915094  0.937198  0.024908\n",
              "3     0.964286     0.936842   0.990099   0.988889  0.936842  0.962162  0.025436\n",
              "4     0.913265     0.870968   0.951456   0.941860  0.870968  0.905028  0.034005\n",
              "5     0.933673     0.891089   0.978947   0.978261  0.891089  0.932642  0.025727\n",
              "mean  0.932653     0.895139   0.969329   0.966738  0.895139  0.929429  0.027231"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sl7He470pXe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "067bfc29-278c-4115-8d72-72a0577e9255"
      },
      "source": [
        "result_SKLearn = run_skenarion_1(clf_DT,X_new,y_new,5)\n",
        "result_SKLearn"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Akurasi</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specifity</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>Waktu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.862245</td>\n",
              "      <td>0.840426</td>\n",
              "      <td>0.882353</td>\n",
              "      <td>0.868132</td>\n",
              "      <td>0.840426</td>\n",
              "      <td>0.854054</td>\n",
              "      <td>0.019585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.882653</td>\n",
              "      <td>0.886792</td>\n",
              "      <td>0.877778</td>\n",
              "      <td>0.895238</td>\n",
              "      <td>0.886792</td>\n",
              "      <td>0.890995</td>\n",
              "      <td>0.013235</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.923469</td>\n",
              "      <td>0.926316</td>\n",
              "      <td>0.920792</td>\n",
              "      <td>0.916667</td>\n",
              "      <td>0.926316</td>\n",
              "      <td>0.921466</td>\n",
              "      <td>0.017233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.882653</td>\n",
              "      <td>0.903226</td>\n",
              "      <td>0.864078</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.903226</td>\n",
              "      <td>0.879581</td>\n",
              "      <td>0.015846</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.918367</td>\n",
              "      <td>0.940594</td>\n",
              "      <td>0.894737</td>\n",
              "      <td>0.904762</td>\n",
              "      <td>0.940594</td>\n",
              "      <td>0.922330</td>\n",
              "      <td>0.015790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.893878</td>\n",
              "      <td>0.899471</td>\n",
              "      <td>0.887947</td>\n",
              "      <td>0.888388</td>\n",
              "      <td>0.899471</td>\n",
              "      <td>0.893685</td>\n",
              "      <td>0.016338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Akurasi  Sensitivity  Specifity  precision    recall  f1_score     Waktu\n",
              "1     0.862245     0.840426   0.882353   0.868132  0.840426  0.854054  0.019585\n",
              "2     0.882653     0.886792   0.877778   0.895238  0.886792  0.890995  0.013235\n",
              "3     0.923469     0.926316   0.920792   0.916667  0.926316  0.921466  0.017233\n",
              "4     0.882653     0.903226   0.864078   0.857143  0.903226  0.879581  0.015846\n",
              "5     0.918367     0.940594   0.894737   0.904762  0.940594  0.922330  0.015790\n",
              "mean  0.893878     0.899471   0.887947   0.888388  0.899471  0.893685  0.016338"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orHWLKtp0pXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Classifier Libraries\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "    \n",
        "clf_RF = RandomForestClassifier(random_state=1,n_estimators=100)\n",
        "clf_AB = AdaBoostClassifier(random_state=1,n_estimators=100)\n",
        "clf_B = BaggingClassifier(random_state=1,n_estimators=100)\n",
        "clf_ET = ExtraTreesClassifier(random_state=1,n_estimators=100)\n",
        "clf_GB = GradientBoostingClassifier(random_state=1,n_estimators=50)\n",
        "\n",
        "##skenario \n",
        "skenario_1 = {}\n",
        "skenario_1['1'] = clf_RF\n",
        "skenario_1['2'] = clf_AB\n",
        "skenario_1['3'] = clf_B\n",
        "skenario_1['4'] = clf_ET\n",
        "skenario_1['5'] = clf_GB\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6xA_Lfu0pX0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for i in skenario_1:\n",
        "    \n",
        "#     clf_skenario = skenario_1[i]\n",
        "#     print(skenario_1[i])\n",
        "\n",
        "    \n",
        "#     result_SKLearn = run_skenarion_1(clf_skenario,X_new,y_new,5)\n",
        "# #     print(result_SKLearn)\n",
        "    \n",
        "#     nama_file = 'hasil/skenario1/random_ensemble/komposisi999.csv'\n",
        "#     nama_file = nama_file.replace('999',str(i))\n",
        "\n",
        "#     result_SKLearn.to_csv(nama_file, sep=';',index=False)\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbU_vQrd2yi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = {}\n",
        "for i in skenario_1:\n",
        "#     print('komposisi', skenario_1[i])\n",
        "  \n",
        "    clf_skenario = skenario_1[i]\n",
        "\n",
        "#     result_SKLearn = run_skenarion_1(clf_skenario,X_new,y_new,5)\n",
        "#     print(result_SKLearn)\n",
        "\n",
        "    result[i] = run_skenarion_1(clf_skenario,X_new,y_new,5)\n",
        "    \n",
        "#     nama_file = 'komposisi999.csv'\n",
        "#     nama_file = nama_file.replace('999',str(i))\n",
        "\n",
        "#     result_SKLearn.to_csv(nama_file, sep=';',index=False)\n",
        "#     !cp komposisi1.csv drive/My\\ Drive/\n",
        "                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RMtVQm90pX-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_result = pd.DataFrame()\n",
        "# for i in skenario_1:\n",
        "#     nama_file = 'hasil/skenario1/random_ensemble/komposisi999.csv'\n",
        "#     nama_file = nama_file.replace('999',str(i))\n",
        "#     df_komposisi = pd.read_csv(nama_file,sep=\";\")\n",
        "#     df_result = df_result.append(df_komposisi.iloc[5:,:])\n",
        "    \n",
        "# df_result.insert(loc=0, column='No', value=list(range(df_result.shape[0])))\n",
        "# df_result = df_result.set_index('No')\n",
        "\n",
        "# del df_result.index.name\n",
        "\n",
        "# df_result.to_csv('hasil/skenario1/random_ensemble/result.csv', sep=';',index=False)\n",
        "# df_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idcF61oH2472",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "1c512aa9-85ef-4216-e329-3699b3fb356b"
      },
      "source": [
        "df_result = pd.DataFrame()\n",
        "for i in skenario_1:\n",
        "    df_result = df_result.append(result[i].iloc[5:,:])\n",
        "    \n",
        "df_result.insert(loc=0, column='No', value=list(range(df_result.shape[0])))\n",
        "df_result = df_result.set_index('No')\n",
        "\n",
        "del df_result.index.name\n",
        "\n",
        "df_result"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Akurasi</th>\n",
              "      <th>Sensitivity</th>\n",
              "      <th>Specifity</th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>Waktu</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.943878</td>\n",
              "      <td>0.903457</td>\n",
              "      <td>0.983162</td>\n",
              "      <td>0.982676</td>\n",
              "      <td>0.903457</td>\n",
              "      <td>0.941141</td>\n",
              "      <td>0.244909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.941837</td>\n",
              "      <td>0.922023</td>\n",
              "      <td>0.961668</td>\n",
              "      <td>0.959345</td>\n",
              "      <td>0.922023</td>\n",
              "      <td>0.940110</td>\n",
              "      <td>0.386847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.938776</td>\n",
              "      <td>0.909382</td>\n",
              "      <td>0.967252</td>\n",
              "      <td>0.965240</td>\n",
              "      <td>0.909382</td>\n",
              "      <td>0.936269</td>\n",
              "      <td>0.991290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.935714</td>\n",
              "      <td>0.888951</td>\n",
              "      <td>0.981260</td>\n",
              "      <td>0.980257</td>\n",
              "      <td>0.888951</td>\n",
              "      <td>0.931993</td>\n",
              "      <td>0.127126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.943878</td>\n",
              "      <td>0.913342</td>\n",
              "      <td>0.972970</td>\n",
              "      <td>0.972288</td>\n",
              "      <td>0.913342</td>\n",
              "      <td>0.941571</td>\n",
              "      <td>0.136509</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Akurasi  Sensitivity  Specifity  precision    recall  f1_score     Waktu\n",
              "0  0.943878     0.903457   0.983162   0.982676  0.903457  0.941141  0.244909\n",
              "1  0.941837     0.922023   0.961668   0.959345  0.922023  0.940110  0.386847\n",
              "2  0.938776     0.909382   0.967252   0.965240  0.909382  0.936269  0.991290\n",
              "3  0.935714     0.888951   0.981260   0.980257  0.888951  0.931993  0.127126\n",
              "4  0.943878     0.913342   0.972970   0.972288  0.913342  0.941571  0.136509"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OzAAnth0pYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importances = clf.feature_importances_\n",
        "# indices = np.argsort(importances)[::-1]\n",
        "# fitur_importance =[]\n",
        "# for f in range(X_new.shape[1]):\n",
        "#     if  importances[indices[f]] > 0 :\n",
        "#         fitur_importance.append([X_new.columns[indices[f]],importances[indices[f]]])\n",
        "#         print(\"%2d) %-*s %f\" % (f + 1, 30,\n",
        "#         X_new.columns[indices[f]],\n",
        "#         importances[indices[f]]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMtobrSQ0pYN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_FI = pd.DataFrame(data=fitur_importance,columns=['fitur','value'])\n",
        "# # np.array(df_FI['fitur'])\n",
        "# df_FI"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrEYG1rS0pYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dot_data = StringIO()  \n",
        "# tree.export_graphviz(clf.estimators_[8], out_file=dot_data,  \n",
        "#                          feature_names=X_new.columns)  \n",
        "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "# Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnK3ml2W0pYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dot_data = StringIO()  \n",
        "# tree.export_graphviz(clf_DT,out_file=dot_data,feature_names=X_new.columns)  \n",
        "# graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "# Image(graph.create_png())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLCBXkiu0pYg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "b54ed4c4-9eea-420b-8d2f-919f20f1da8c"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# get some data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# build a classifier\n",
        "clf = RandomForestClassifier(random_state=1,n_jobs=-1)\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\"max_depth\": [5,9,11,15 ],\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": sp_randint(1, 11),\n",
        "#               \"min_samples_split\": sp_randint(2, 11),\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5, iid=False,random_state=1)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X_new, y_new)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid = {\"max_depth\": [5,9,11,15 ],\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": [1, 3, 10],\n",
        "#               \"min_samples_split\": [2, 3, 10],\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)\n",
        "start = time()\n",
        "grid_search.fit(X_new, y_new)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "RandomizedSearchCV took 26.30 seconds for 20 candidates parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.945 (std: 0.004)\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 11}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.945 (std: 0.002)\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 9}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.944 (std: 0.004)\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 15}\n",
            "\n",
            "GridSearchCV took 24.69 seconds for 20 candidate parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.945 (std: 0.004)\n",
            "Parameters: {'max_depth': 11, 'n_estimators': 100}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.945 (std: 0.002)\n",
            "Parameters: {'max_depth': 9, 'n_estimators': 100}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.944 (std: 0.004)\n",
            "Parameters: {'max_depth': 15, 'n_estimators': 100}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV8J9P2i0pZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "outputId": "f6816f9b-02be-48be-8f26-09fb0f1ab56d"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# get some data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# build a classifier\n",
        "clf = AdaBoostClassifier(random_state=1)\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": sp_randint(1, 11),\n",
        "#               \"min_samples_split\": sp_randint(2, 11),\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5, iid=False,random_state=1)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X_new, y_new)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid = {\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": [1, 3, 10],\n",
        "#               \"min_samples_split\": [2, 3, 10],\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)\n",
        "start = time()\n",
        "grid_search.fit(X_new, y_new)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py:266: UserWarning: The total space of parameters 5 is smaller than n_iter=20. Running 5 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "RandomizedSearchCV took 5.43 seconds for 20 candidates parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.936 (std: 0.012)\n",
            "Parameters: {'n_estimators': 10}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.933 (std: 0.012)\n",
            "Parameters: {'n_estimators': 75}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.933 (std: 0.020)\n",
            "Parameters: {'n_estimators': 100}\n",
            "\n",
            "GridSearchCV took 5.38 seconds for 5 candidate parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.936 (std: 0.012)\n",
            "Parameters: {'n_estimators': 10}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.933 (std: 0.012)\n",
            "Parameters: {'n_estimators': 75}\n",
            "\n",
            "Model with rank: 3\n",
            "Mean validation score: 0.933 (std: 0.020)\n",
            "Parameters: {'n_estimators': 100}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIRTSU6d0pZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "74c8a817-44b7-4633-d932-2dddc3afb02c"
      },
      "source": [
        "print(__doc__)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from time import time\n",
        "from scipy.stats import randint as sp_randint\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# get some data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n",
        "\n",
        "# build a classifier\n",
        "clf = GradientBoostingClassifier(random_state=1)\n",
        "\n",
        "\n",
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")\n",
        "\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\"max_depth\": [3,5,9,11,15 ],\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": sp_randint(1, 11),\n",
        "#               \"min_samples_split\": sp_randint(2, 11),\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search, cv=5, iid=False,random_state=1)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X_new, y_new)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n",
        "# use a full grid over all parameters\n",
        "param_grid = {\"max_depth\": [3,5,9,11,15 ],\n",
        "              'n_estimators' : [10,25,50,75,100],\n",
        "#               \"max_features\": [1, 3, 10],\n",
        "#               \"min_samples_split\": [2, 3, 10],\n",
        "#               \"bootstrap\": [True, False],\n",
        "#               \"criterion\": [\"gini\", \"entropy\"]\n",
        "             }\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, iid=False)\n",
        "start = time()\n",
        "grid_search.fit(X_new, y_new)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "RandomizedSearchCV took 24.71 seconds for 20 candidates parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.938 (std: 0.011)\n",
            "Parameters: {'n_estimators': 50, 'max_depth': 3}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.937 (std: 0.012)\n",
            "Parameters: {'n_estimators': 75, 'max_depth': 3}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.937 (std: 0.010)\n",
            "Parameters: {'n_estimators': 100, 'max_depth': 3}\n",
            "\n",
            "GridSearchCV took 31.21 seconds for 25 candidate parameter settings.\n",
            "Model with rank: 1\n",
            "Mean validation score: 0.938 (std: 0.011)\n",
            "Parameters: {'max_depth': 3, 'n_estimators': 50}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.937 (std: 0.012)\n",
            "Parameters: {'max_depth': 3, 'n_estimators': 75}\n",
            "\n",
            "Model with rank: 2\n",
            "Mean validation score: 0.937 (std: 0.010)\n",
            "Parameters: {'max_depth': 3, 'n_estimators': 100}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_X2Z2m50pZg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12mf3c2I0pZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4CZf2H10pZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}